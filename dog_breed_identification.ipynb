{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efa260a2-4299-4097-bce8-3cb1ef6d9f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a73f0d0-f9d8-4e45-b4ac-2bc4c5dba3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogDataset(Dataset):\n",
    "    def __init__(self, pd, folder, transform):  #초기화\n",
    "        self.pd_table = pd\n",
    "        self.img_dir = folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):  #image load\n",
    "        img_path = self.img_dir + self.pd_table.file_name[index] + \".jpg\"\n",
    "        img = Image.open(img_path)\n",
    "        label = self.pd_table.encoded[index]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pd_table)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "330dac5e-84fe-4821-a626-b2ba8f62d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 29 * 29, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 120)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #print(x.size())\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #print(x.size())\n",
    "        x = torch.flatten(x, 1) # 배치를 제외한 모든 차원을 평탄화(flatten)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "'''\n",
    "\n",
    "#model = Net()\n",
    "model = models.resnet18(pretrained = True)\n",
    "\n",
    "num_classes = 120\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1390a99f-e92c-4ccd-b9f8-aec8e882a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize([image_size, image_size]),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean = (0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cb1f8df-a476-45a1-bb48-31e113f2d212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "pd_table = pd.read_csv(\"D:/code/dog-breed-identification/labels.csv\", names = ['file_name', 'label'], header = 0)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "targets = le.fit_transform(pd_table['label'])\n",
    "pd_table['encoded'] = targets\n",
    "print(pd_table['encoded'].max())\n",
    "\n",
    "train_dataset = DogDataset(pd = pd_table, folder = \"D:/code/dog-breed-identification/train/\", transform = transform)\n",
    "train_size = int(len(train_dataset)*0.8)\n",
    "validation_size = len(train_dataset) - train_size\n",
    "\n",
    "train_data, validation_data = random_split(train_dataset, [train_size, validation_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = 16, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size = 16, shuffle=True)\n",
    "\n",
    "#print(train_data.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac89f3b6-9332-4c36-bd68-01ceb0a4291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "#from torchsummary import summary\n",
    "#summary(model, input_size=(3, image_size, image_size))\n",
    "\n",
    "num_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16200a-418c-47dc-84bb-c259caacfb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 train_loss:2.352 validation_loss:2.278 val_accuracy: 0.400 best_loss:2.278\n",
      "epoch:2 train_loss:1.458 validation_loss:2.035 val_accuracy: 0.450 best_loss:2.035\n",
      "epoch:3 train_loss:1.012 validation_loss:2.102 val_accuracy: 0.445 best_loss:2.035\n",
      "epoch:4 train_loss:0.672 validation_loss:2.533 val_accuracy: 0.419 best_loss:2.035\n",
      "epoch:5 train_loss:0.500 validation_loss:3.221 val_accuracy: 0.353 best_loss:2.035\n",
      "epoch:6 train_loss:0.430 validation_loss:3.030 val_accuracy: 0.367 best_loss:2.035\n",
      "epoch:7 train_loss:0.369 validation_loss:2.843 val_accuracy: 0.403 best_loss:2.035\n",
      "epoch:8 train_loss:0.403 validation_loss:3.459 val_accuracy: 0.363 best_loss:2.035\n",
      "epoch:9 train_loss:0.306 validation_loss:3.340 val_accuracy: 0.368 best_loss:2.035\n",
      "epoch:10 train_loss:0.256 validation_loss:3.115 val_accuracy: 0.359 best_loss:2.035\n",
      "epoch:11 train_loss:0.276 validation_loss:3.293 val_accuracy: 0.345 best_loss:2.035\n",
      "epoch:12 train_loss:0.296 validation_loss:3.402 val_accuracy: 0.347 best_loss:2.035\n",
      "epoch:13 train_loss:0.236 validation_loss:3.245 val_accuracy: 0.358 best_loss:2.035\n",
      "epoch:14 train_loss:0.269 validation_loss:3.500 val_accuracy: 0.335 best_loss:2.035\n",
      "epoch:15 train_loss:0.249 validation_loss:3.448 val_accuracy: 0.368 best_loss:2.035\n",
      "epoch:16 train_loss:0.206 validation_loss:3.295 val_accuracy: 0.384 best_loss:2.035\n",
      "epoch:17 train_loss:0.281 validation_loss:3.389 val_accuracy: 0.351 best_loss:2.035\n",
      "epoch:18 train_loss:0.215 validation_loss:3.392 val_accuracy: 0.345 best_loss:2.035\n",
      "epoch:19 train_loss:0.180 validation_loss:4.353 val_accuracy: 0.278 best_loss:2.035\n",
      "epoch:20 train_loss:0.252 validation_loss:3.842 val_accuracy: 0.355 best_loss:2.035\n",
      "epoch:21 train_loss:0.225 validation_loss:3.472 val_accuracy: 0.367 best_loss:2.035\n",
      "epoch:22 train_loss:0.223 validation_loss:3.468 val_accuracy: 0.384 best_loss:2.035\n",
      "epoch:23 train_loss:0.173 validation_loss:3.307 val_accuracy: 0.353 best_loss:2.035\n",
      "epoch:24 train_loss:0.234 validation_loss:3.334 val_accuracy: 0.380 best_loss:2.035\n",
      "epoch:25 train_loss:0.219 validation_loss:3.271 val_accuracy: 0.393 best_loss:2.035\n",
      "epoch:26 train_loss:0.165 validation_loss:2.789 val_accuracy: 0.392 best_loss:2.035\n",
      "epoch:27 train_loss:0.188 validation_loss:3.442 val_accuracy: 0.343 best_loss:2.035\n",
      "epoch:28 train_loss:0.236 validation_loss:3.282 val_accuracy: 0.374 best_loss:2.035\n",
      "epoch:29 train_loss:0.162 validation_loss:3.177 val_accuracy: 0.379 best_loss:2.035\n",
      "epoch:30 train_loss:0.149 validation_loss:3.149 val_accuracy: 0.376 best_loss:2.035\n",
      "epoch:31 train_loss:0.152 validation_loss:4.402 val_accuracy: 0.293 best_loss:2.035\n"
     ]
    }
   ],
   "source": [
    "best_loss = 10\n",
    "\n",
    "for epoch in range(num_epoch):   # 데이터셋을 수차례 반복합니다.\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x,y in train_loader:\n",
    "        x = x.type(torch.FloatTensor)\n",
    "        x = x.to(device)\n",
    "        y = y.type(torch.LongTensor)\n",
    "        y = y.to(device)\n",
    "\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    \n",
    "    validation_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for x, y in validation_loader:\n",
    "            x = x.type(torch.FloatTensor)\n",
    "            x = x.to(device)\n",
    "            y = y.type(torch.LongTensor)\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            validation_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y.size(0)\n",
    "            accuracy += (predicted == y).sum().item()\n",
    "            \n",
    "        validation_loss = validation_loss/len(validation_loader)\n",
    "        accuracy = accuracy/total\n",
    "        \n",
    "    if validation_loss < best_loss:\n",
    "        best_loss = validation_loss\n",
    "        torch.save(model, 'D:/code/dog-breed-identification/my_model.pt')\n",
    "        \n",
    "    print('epoch:' + str(epoch+1) + ' train_loss:%.3f validation_loss:%.3f val_accuracy: %.3f best_loss:%.3f'%(train_loss,validation_loss, accuracy, best_loss))        \n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe8edf54-2b25-4a70-815b-067aaa367fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(predicted == y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
